{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project IMDB MOVIE REVIEW\n",
    "\n",
    "The IMDB dataset comprises 25,000 movie reviews, making it a valuable resource for natural language processing and text analytics. This dataset surpasses previous benchmark datasets in terms of its size and contains a binary sentiment classification. Specifically, it consists of 12,500 highly polar movie reviews for training and an additional 12,500 for testing purposes.\n",
    "\n",
    "The primary objective of this project is to accurately predict the number of positive and negative reviews by utilizing classification techniques.\n",
    "\n",
    "In summary, this dataset provides a comprehensive and robust platform for analyzing movie reviews and developing classification models to accurately predict sentiment.\n",
    "\n",
    "The project will involve a series of fundamental procedures, which include the following steps:\n",
    "\n",
    "- Preprocess Text Data(Remove punctuation, Perform Tokenization, Remove stopwords and Lemmatize/Stem)\n",
    "- Perform TFIDF Vectorization\n",
    "- Exploring parameter settings using GridSearchCV on Random Forest & Gradient Boosting Classifier\n",
    "- Perform Final evaluation of models on the best parameter settings using the evaluation metrics\n",
    "- Report the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this movie when I was about 12 when it c...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So im not a big fan of Boll's work but then ag...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           body_text     label\n",
       "0  This show was an amazing, fresh & innovative i...  negative\n",
       "1  Encouraged by the positive comments about this...  negative\n",
       "2  Phil the Alien is one of those quirky films wh...  negative\n",
       "3  I saw this movie when I was about 12 when it c...  negative\n",
       "4  So im not a big fan of Boll's work but then ag...  negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('IMDB_dataset_reduced.csv')\n",
    "data.columns = ['body_text','label']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data has 6000 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "# Shape of the dataset\n",
    "\n",
    "print(\"Input data has {} rows and {} columns\".format(len(data), len(data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 6000 rows, 3000 are positive, 3000 are negative\n"
     ]
    }
   ],
   "source": [
    "# Positive/negative values\n",
    "\n",
    "print(\"Out of {} rows, {} are positive, {} are negative\".format(len(data),\n",
    "                                                       len(data[data['label']=='positive']),\n",
    "                                                       len(data[data['label']=='negative'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null in label: 0\n",
      "Number of null in text: 0\n"
     ]
    }
   ],
   "source": [
    "# How much missing data is there?\n",
    "\n",
    "print(\"Number of null in label: {}\".format(data['label'].isnull().sum()))\n",
    "print(\"Number of null in text: {}\".format(data['body_text'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the analysis of data, its observed that there are no null text and label.Also,dataset have equal no of positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is counting the percentage of punctations in each text\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        761\n",
       "1        552\n",
       "2        483\n",
       "3        758\n",
       "4       1830\n",
       "        ... \n",
       "5995     728\n",
       "5996    2583\n",
       "5997     448\n",
       "5998    2027\n",
       "5999    1209\n",
       "Name: body_len, Length: 6000, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the no of characters and punctuation percentage in each text \n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "data['body_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The clean_text function is removing the punctuations then tokenizing.Later,the stopwords are removed and stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) # We are removing punctuations here\n",
    "    \n",
    "    tokens = re.split('\\W+', text) # We are tokenizing the data here by using Regex\n",
    "    \n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords] \n",
    "    \n",
    "#Removing stopwords removed then passing the token to porter stemmer for stemming\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "X_tfidf_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "\n",
    "# CountVectorizer\n",
    "# count_vect = CountVectorizer(analyzer=clean_text)\n",
    "# X_count = count_vect.fit_transform(data['body_text'])\n",
    "# X_count_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_count.toarray())], axis=1)\n",
    "\n",
    "# X_count_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring parameter settings using GridSearchCV for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.729252</td>\n",
       "      <td>0.893545</td>\n",
       "      <td>0.724543</td>\n",
       "      <td>0.116980</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 100}</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.860833</td>\n",
       "      <td>0.835833</td>\n",
       "      <td>0.844167</td>\n",
       "      <td>0.839500</td>\n",
       "      <td>0.013256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.966633</td>\n",
       "      <td>0.505469</td>\n",
       "      <td>0.631164</td>\n",
       "      <td>0.039127</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 100}</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.824167</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.829667</td>\n",
       "      <td>0.010809</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.120791</td>\n",
       "      <td>0.724328</td>\n",
       "      <td>0.581538</td>\n",
       "      <td>0.078089</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.822500</td>\n",
       "      <td>0.827500</td>\n",
       "      <td>0.809167</td>\n",
       "      <td>0.839167</td>\n",
       "      <td>0.823167</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.301973</td>\n",
       "      <td>0.562558</td>\n",
       "      <td>0.542519</td>\n",
       "      <td>0.068690</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 50}</td>\n",
       "      <td>0.798333</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.827500</td>\n",
       "      <td>0.815167</td>\n",
       "      <td>0.011086</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.591951</td>\n",
       "      <td>0.263456</td>\n",
       "      <td>0.550727</td>\n",
       "      <td>0.045098</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 50}</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.827500</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.018311</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "5      10.729252      0.893545         0.724543        0.116980   \n",
       "8      11.966633      0.505469         0.631164        0.039127   \n",
       "2       7.120791      0.724328         0.581538        0.078089   \n",
       "7       8.301973      0.562558         0.542519        0.068690   \n",
       "4       7.591951      0.263456         0.550727        0.045098   \n",
       "\n",
       "  param_max_depth param_n_estimators                                  params  \\\n",
       "5              20                100  {'max_depth': 20, 'n_estimators': 100}   \n",
       "8              30                100  {'max_depth': 30, 'n_estimators': 100}   \n",
       "2              10                100  {'max_depth': 10, 'n_estimators': 100}   \n",
       "7              30                 50   {'max_depth': 30, 'n_estimators': 50}   \n",
       "4              20                 50   {'max_depth': 20, 'n_estimators': 50}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "5           0.820000           0.836667           0.860833           0.835833   \n",
       "8           0.817500           0.821667           0.840000           0.824167   \n",
       "2           0.817500           0.822500           0.827500           0.809167   \n",
       "7           0.798333           0.815000           0.826667           0.808333   \n",
       "4           0.786667           0.815000           0.837500           0.800000   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "5           0.844167         0.839500        0.013256                1  \n",
       "8           0.845000         0.829667        0.010809                2  \n",
       "2           0.839167         0.823167        0.010033                3  \n",
       "7           0.827500         0.815167        0.011086                4  \n",
       "4           0.827500         0.813333        0.018311                5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "param = {'n_estimators': [10, 50, 100],\n",
    "        'max_depth': [10, 20, 30]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5)\n",
    "gs_fit = gs.fit(X_tfidf_feat, data['label'])\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After performing Grid Search CV for Random Forest we can see that best performing parameters are\n",
    "# n_estimators -> 100\n",
    "# max_depth -> 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring parameter setting using GradientBoostingClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63.326527</td>\n",
       "      <td>0.474770</td>\n",
       "      <td>0.788108</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 11, 'n_est...</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.765833</td>\n",
       "      <td>0.740833</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.7465</td>\n",
       "      <td>0.013117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.059748</td>\n",
       "      <td>0.734426</td>\n",
       "      <td>0.815267</td>\n",
       "      <td>0.031587</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.748333</td>\n",
       "      <td>0.759167</td>\n",
       "      <td>0.736667</td>\n",
       "      <td>0.780833</td>\n",
       "      <td>0.7460</td>\n",
       "      <td>0.025141</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.276588</td>\n",
       "      <td>0.204028</td>\n",
       "      <td>0.746569</td>\n",
       "      <td>0.030640</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'n_esti...</td>\n",
       "      <td>0.694167</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.704167</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.7180</td>\n",
       "      <td>0.024035</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "2      63.326527      0.474770         0.788108        0.009424   \n",
       "1      46.059748      0.734426         0.815267        0.031587   \n",
       "0      23.276588      0.204028         0.746569        0.030640   \n",
       "\n",
       "  param_learning_rate param_max_depth param_n_estimators  \\\n",
       "2                 0.1              11                 10   \n",
       "1                 0.1               7                 10   \n",
       "0                 0.1               3                 10   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "2  {'learning_rate': 0.1, 'max_depth': 11, 'n_est...           0.726667   \n",
       "1  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...           0.705000   \n",
       "0  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...           0.694167   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "2           0.745000           0.765833           0.740833           0.754167   \n",
       "1           0.748333           0.759167           0.736667           0.780833   \n",
       "0           0.702500           0.729167           0.704167           0.760000   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "2           0.7465        0.013117                1  \n",
       "1           0.7460        0.025141                2  \n",
       "0           0.7180        0.024035                3  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [10], \n",
    "    'max_depth': [3, 7, 11],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(gb, param, cv=5)\n",
    "cv_fit = clf.fit(X_tfidf_feat, data['label'])\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After performing Grid Search CV for Gradient Boosting we can see that best performing parameters are:\n",
    "# n_estimators -> 10\n",
    "# max_depth -> 11\n",
    "#learning_rate -> 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting to train and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['body_text', 'body_len', 'punct%']], data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>36750</th>\n",
       "      <th>36751</th>\n",
       "      <th>36752</th>\n",
       "      <th>36753</th>\n",
       "      <th>36754</th>\n",
       "      <th>36755</th>\n",
       "      <th>36756</th>\n",
       "      <th>36757</th>\n",
       "      <th>36758</th>\n",
       "      <th>36759</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>594</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>560</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>691</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1842</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>577</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36762 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%    0    1    2    3    4    5    6    7  ...  36750  \\\n",
       "0       594     7.6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "1       560     3.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "2       691     3.9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "3      1842     3.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "4       577     4.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "\n",
       "   36751  36752  36753  36754  36755  36756  36757  36758  36759  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 36762 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizing the text\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n",
    "\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
    "\n",
    "X_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "X_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
    "X_train_vect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final evaluation of models\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 6.375 / Predict time: 0.609 ---- Precision: 0.839 / Recall: 0.852 / Accuracy: 0.849\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=30, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='positive', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 60.901 / Predict time: 0.569 ---- Precision: 0.712 / Recall: 0.79 / Accuracy: 0.743\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=10, max_depth=11)\n",
    "\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='positive', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest is best performing model with accuracy of 84.9 %"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
